<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.0.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" integrity="sha256-yIDrPSXHZdOZhAqiBP7CKzIwMQmRCJ8UeB8Jo17YC4o=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"rookiedb0901.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.19.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="深夜食堂">
<meta property="og:url" content="https://rookiedb0901.github.io/index.html">
<meta property="og:site_name" content="深夜食堂">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="DB">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://rookiedb0901.github.io/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>深夜食堂</title>
  







<link rel="stylesheet" href="https://lib.baomitu.com/social-share.js/1.0.16/css/share.min.css">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">深夜食堂</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">DB</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">54</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">20</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://rookiedb0901.github.io/2024/10/09/%E8%A1%8C%E4%B8%BA%E5%BA%8F%E5%88%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="DB">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="深夜食堂">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 深夜食堂">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/10/09/%E8%A1%8C%E4%B8%BA%E5%BA%8F%E5%88%97/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-10-09 18:32:59" itemprop="dateCreated datePublished" datetime="2024-10-09T18:32:59+08:00">2024-10-09</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="lastn">LastN</h1>
<p>LastN，即用户最近 n
次点击、点赞、收藏、转发等行为是推荐系统中重要的特征，可以帮助召回和排序变得更精准。</p>
<p>用户的LastN序列是用户最近交互过的N个物品，将这些物品的ID
embedding成向量并取平均得到一个向量，作为用户的一种特征，和其他特征一起输入神经网络。</p>
<p>这种方法适用于双塔模型、三塔模型、精排模型。</p>
<p>在实践中，可以用点击、点赞、收藏等的LastN序列分别生成一个特征向量，再把这些向量拼接起来。除了使用物品ID，还可以使用物品其他特征做embedding，再和ID的embedding拼接起来。</p>
<h1 id="din模型">DIN模型</h1>
<p>DIN模型是对LastN序列建模的一种方法，效果优于简单的平均。DIN
的本质是注意力机制。</p>
<p>DIN采用了加权平均代替普通平均，权重是候选物品向量与LastN物品向量的相似度。</p>
<p>DIN模型只用于精排模型，而不用于双塔模型、三塔模型。这是因为双塔模型中物品表征离线存储，用户塔无法看到候选物品特征。</p>
<h1 id="sim模型">SIM模型</h1>
<p>DIN模型的计算量受N影响，因此N的规模不能太大，一般只能为几百，导致模型无法看到用户的长期兴趣。</p>
<p>SIM模型的目标是保留用户的长期行为序列，而且计算量不会过大。</p>
<p>SIM模型对DIN做出改进：</p>
<ul>
<li>DIN对LastN向量做加权平均，权重是相似度。</li>
<li>如果某LastN物品与候选物品差异很大，则权重接近零。</li>
<li>可以快速排除掉与候选物品无关的LastN物品，降低注意层的计算量。</li>
</ul>
<p>SIM的步骤如下：</p>
<p><strong>查找</strong>：</p>
<ul>
<li>Hard Search：保留LastN物品中与候选物品类目相同的。</li>
<li>Soft
Search：最近邻查找，将候选物品向量作为query查找LastN中k个相似度最高的。</li>
<li>前者更快，后者效果更好。一般用前者就行。</li>
</ul>
<p><strong>注意力机制</strong>：</p>
<ul>
<li>将DIN中的LastN向量换成TopK向量即可。</li>
<li>SIM记录用户的长期行为，LastN可能存在很久之前的交互，因此可以加入时间信息：
<ul>
<li>用户与某个LastN物品的交互时刻距今为δ；</li>
<li>对δ做离散化，在做embedding，变成向量<strong>d</strong>；</li>
<li>将物品向量<strong>x</strong>和时间向量<strong>d</strong>拼接在一起，作为一个LastN物品的表征。</li>
</ul></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://rookiedb0901.github.io/2024/10/09/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="DB">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="深夜食堂">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 深夜食堂">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/10/09/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-10-09 18:32:59" itemprop="dateCreated datePublished" datetime="2024-10-09T18:32:59+08:00">2024-10-09</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>“协同过滤”就是协同⼤家的反馈、评价和意见⼀起对海量的信息进⾏过滤，从中筛选出⽬标⽤户可能感兴趣的信息的推荐过程。</p>
<ul>
<li><strong>共现矩阵</strong>：用横坐标表示物品，纵坐标表示用户，矩阵元素值表示用户对物品的感兴趣程度。</li>
</ul>
<h1 id="基于用户的协同过滤">基于用户的协同过滤</h1>
<p>基于用户的协同过滤（userCF）向用户推荐相似用户感兴趣的物品。在为一名用户推荐物品时，首先找到与该用户相似度最高的n个用户，根据共现矩阵计算这些用户对每个物品的感兴趣程度，再将各个物品的得分进行排序，选择得分最高的一系列物品进行推荐。</p>
<p>以上总结了userCF的总体逻辑，但关于用户相似度的计算排序的过程需要可量化的方式。以下进行详细介绍。</p>
<h2 id="用户相似度计算">用户相似度计算</h2>
<p>我们可用共现矩阵中的行向量表示相应用户的用户向量（共现矩阵行向量中的每个值表示了该用户对一个物品的感兴趣程度），这样计算用户相似度便转化为了计算向量相似度，两个向量之间常⽤的相似度计算⽅法有如下⼏种。</p>
<h3 id="余弦相似度">余弦相似度</h3>
<p>余弦相似度衡量了用户向量i和用户向量j之间的夹角大小，夹角越小则相似度越大。
<span class="math display">\[
\begin{equation}
sim(i,j)=cos(i,j)= \frac{i·j}{||i||·||j||}  
\end{equation}
\]</span></p>
<h3 id="皮尔逊相关系数">皮尔逊相关系数</h3>
<p>皮尔逊相关系数使用用户平均分对各独立评分进行修正，减小用户评分偏置的影响。
<span class="math display">\[
sim(i,j)=\frac{\sum_{p∈P}(R_{i,p}-\overline{R_i})(R_{j,p}-\overline{R_j})}{\sqrt{\sum_{p∈P}(R_{i,p}-\overline{R_i})^2}\sqrt{\sum_{p∈P}(R_{j,p}-\overline{R_j})^2}}
\]</span> 其中， <span class="math inline">\(R_{i,p}\)</span>
代表⽤户i对物品p的评分。 <span
class="math inline">\(\overline{R_i}\)</span>
代表⽤户i对所有物品的平均评分，P代表所有物品的集合。</p>
<p>以上使用的是用户平均分，同理也可引入物品平均分，不再赘述。</p>
<h2 id="最终结果排序">最终结果排序</h2>
<p>一般利⽤⽤户相似度和相似⽤户的评价的加权平均获得⽬标⽤户的评价预测。
<span class="math display">\[
R_{u,p}=\frac{\sum_{s∈S}(w_{u,s}·R_{s,p})}{\sum_{s∈S}w_{u,s}}
\]</span> 其中 <span class="math inline">\(w_{u,s}\)</span>
表示用户u和用户s的相似度。</p>
<p>获得用户p对各个物品的评价预测后，将这些得分进行排序即可得到推荐列表。</p>
<p>userCF存在以下缺点：</p>
<ul>
<li>互联网应用场景下用户数往往非常大，而存储用户相似度矩阵需要n^2的空间复杂度。</li>
<li>用户的历史数据向量往往非常稀疏，对于只有⼏次购买或者点击⾏为的⽤户来说，找到相似⽤户的准确度⾮常低。</li>
</ul>
<h1 id="基于物品的协同过滤">基于物品的协同过滤</h1>
<p>基于物品的协同过滤（ItemCF）通过计算共现矩阵中物品列向量的相似度得到物品之间的相似矩阵，再找到⽤户的历史正反馈物品的相似物品进⾏进⼀步排序和推荐。</p>
<p>ItemCF的具体步骤如下：</p>
<ul>
<li>基于历史数据构建共现矩阵（用户为行，物品为列）</li>
<li>计算两两列向量（即物品向量）的相似性，构建物品相似度矩阵</li>
<li>利用物品相似度矩阵，针对目标用户历史正反馈物品查找TOP K个物品</li>
<li>对于相似物品集合中的物品，利用相似度分值进行排序</li>
</ul>
<p>如果一个物品与多个历史正反馈物品相似，则相似度需要累加： <span
class="math display">\[
R_{u,p}=\sum_{h∈H}(w_{p,h}·R_{u,h})
\]</span>
其中H是目标用户的正反馈物品集合，权重w为物品间相似度，R是用户对物品的已有评分。</p>
<h2 id="swing模型">Swing模型</h2>
<p>Swing模型和ItemCF十分相似，仅在物品相似度的计算上有所不同。Swing模型的相似度计算方式如下：
<span class="math display">\[
sim(i_1,i_2)=\sum_{u_1∈v}\sum_{u_2∈v}\frac{1}{α+overlap(u_1,u_2)}
\]</span>
其中w_1为喜欢物品i_1的用户集合，v为w_1和w_2的交集，overlap计算两个用户喜欢的物品交集的大小，α为参数。</p>
<p>这个相似度计算公式下，v越大两个物品的相似度也越大，引入overlap是为了削弱“小圈子”的影响，例如当两个人同属一个微信群，则被转发到该群的所有链接都可能被两个人同时点开，即便这些链接毫不相关。</p>
<h1 id="总结">总结</h1>
<ul>
<li><p>UserCF基于⽤户相似度进⾏推荐，使其具备更强的社交特性。⽤户能够快速得知与⾃⼰兴趣相似的⼈最近喜欢的是什么，即使某个兴趣点以前不在⾃⼰的兴趣范围内，也有可能通过“朋友”的动态快速更新⾃⼰的推荐列表。这样的特点使其⾮常适⽤于新闻推荐场景。</p></li>
<li><p>ItemCF更适⽤于兴趣变化较为稳定的应⽤，⽤户在⼀个时间段内更倾向于寻找⼀类商品，这时利⽤物品相似度为其推荐相关物品是契合⽤户动机的
，例如电商和视频推荐。</p></li>
</ul>
<p>协同过滤是⼀个⾮常直观、可解释性很强的模型，但它并不具备较强的泛化能⼒。热门的物品具有很强的头部效应，容易跟⼤量物品产⽣相似性；⽽尾部的物品由于特征向量稀疏，很少与其他物品产⽣相似性，导致很少被推荐。</p>
<p>另外，协同过滤仅利⽤⽤户和物品的交互信息，⽆法有效地引⼊⽤户年龄、性别、商品描述、商品分类、当前时间等⼀系列⽤户特征、物品特征和上下⽂特征。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://rookiedb0901.github.io/2024/10/09/%E5%90%91%E9%87%8F%E5%8F%AC%E5%9B%9E/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="DB">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="深夜食堂">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 深夜食堂">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/10/09/%E5%90%91%E9%87%8F%E5%8F%AC%E5%9B%9E/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-10-09 18:32:59" itemprop="dateCreated datePublished" datetime="2024-10-09T18:32:59+08:00">2024-10-09</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="矩阵补充">矩阵补充</h1>
<h2 id="模型训练">模型训练</h2>
<p>矩阵补充模型首先通过两个Embedding矩阵W_1、W_2将用户向量和物品向量映射成低维向量a_u和b_i，再将二者求内积得到用户u对物品i的预估兴趣分数。</p>
<p>训练的目标函数为： <span class="math display">\[
\min_{A,B}\sum_{(u,i,y)∈Ω}(y-&lt;a_u,b_i&gt;)^2
\]</span>
其中y是用户u对物品i的真实兴趣分数。该目标函数的目的是最小化预估兴趣分数和真实兴趣分数的距离，以此训练Embedding矩阵W_1、W_2。训练后，我们便可使用&lt;a_u,b_i&gt;作为兴趣分数来补充共现矩阵中没有产生过交互的用户和物品。</p>
<h2 id="线上服务">线上服务</h2>
<p>训练得到用户和物品的embedding，将embedding的结果存储到key-value表，用户的key-value表中key为用户id，value为embedding后的向量。</p>
<p>线上服务过程如下：</p>
<ul>
<li>把用户id作为key查询用户的向量，记作a。</li>
<li>最邻近查找：查找用户最有可能感兴趣的k个物品，作为召回结果。
<ul>
<li>第i号物品的embedding向量记作b_i。</li>
<li>内积&lt;a,b_i&gt;是用户对第i号物品兴趣的预估。</li>
<li>返回内积最大的k个物品。</li>
</ul></li>
</ul>
<p>但是矩阵补充实际效果并不好，原因如下：</p>
<ul>
<li>为利用物品、用户属性信息。</li>
<li>负样本选取方式不对。</li>
<li>内积不如余弦相似度，平方损失函数不如交叉熵损失。</li>
</ul>
<h1 id="双塔模型">双塔模型</h1>
<p>双塔模型（two-tower）也叫 DSSM，是推荐系统中最重要的召回通道。</p>
<p>双塔模型有两个塔：用户塔、物品塔。两个塔各输出一个向量，作为用户、物品的表征。两个向量的余弦相似度作为对兴趣的预估。</p>
<figure>
<img src="用户塔.png" alt="用户塔" />
<figcaption aria-hidden="true">用户塔</figcaption>
</figure>
<figure>
<img src="物品塔.png" alt="物品塔" />
<figcaption aria-hidden="true">物品塔</figcaption>
</figure>
<h2 id="模型训练-1">模型训练</h2>
<p>将物品样本分为正样本（用户感兴趣的物品）和负样本（用户不感兴趣的物品）。</p>
<ul>
<li><p>Pointwise: 独立看待每个正样本、负样本，做简单的二元分类</p></li>
<li><p>Pairwise: 每次取一个正样本、一个负样本</p></li>
<li><p>Listwise: 每次取一个正样本、多个负样本</p></li>
</ul>
<h3 id="pointwise训练">Pointwise训练</h3>
<p>工业界一般使用这种方法。</p>
<p>把召回看作二分类任务：</p>
<ul>
<li>对于正样本，鼓励cos(a,b)接近+1。</li>
<li>对于负样本，鼓励cos(a,b)接近-1。</li>
</ul>
<p>其中a为用户的表征，b为物品的表征。正负样本数量通常控制在1:2或1:3。</p>
<h3 id="pairwise训练">Pairwise训练</h3>
<p>Pairwise每次取一个正样本、一个负样本。基本想法是要鼓励cos(a,b+)大于cos(a,b-)，b+是正样本的表征，b-是负样本的表征。</p>
<p>损失函数：</p>
<ul>
<li><p>Triplet hinge loss <span class="math display">\[
L(a,b+,b-)=\max\{0,cos(a,b-)+m-cos(a,b+)\}
\]</span></p>
<ul>
<li><p>如果cos(a,b+)大于cos(a,b-)+m，则没有损失</p></li>
<li><p>否则，损失等于cos(a,b-)+m-cos(a,b+)</p></li>
</ul></li>
<li><p>Triplet logistic loss <span class="math display">\[
L(a,b+,b-)=log(1+exp[σ·(cos(a,b-)-cos(a,b+))])
\]</span> σ大于0的超参数。</p></li>
</ul>
<h3 id="listwise训练">Listwise训练</h3>
<p>Listwise每次取一个正样本、多个负样本。这些物品样本与用户表征的余弦相似度经过softmax后，要使正样本尽可能大（接近1），使负样本尽可能小（接近0）。损失函数使用交叉熵损失。</p>
<figure>
<img src="listwise训练.png" alt="listwise训练" />
<figcaption aria-hidden="true">listwise训练</figcaption>
</figure>
<h2 id="正负样本">正负样本</h2>
<h2 id="模型应用">模型应用</h2>
<p>双塔模型训练完成后，通过两个步骤投入实际应用：</p>
<ul>
<li><p><strong>离线存储</strong>：用物品塔计算每个物品的特征向量<strong>b</strong>，把&lt;特征向量<strong>b</strong>,物品ID&gt;保存到向量数据库用作最近邻查找。</p></li>
<li><p><strong>线上召回</strong>：需要推荐时，调用用户塔线上计算用户向量<strong>a</strong>，并用<strong>a</strong>作为query查询向量数据库，查找和<strong>a</strong>余弦相似度最高的k个物品向量，返回这k个物品ID。</p></li>
</ul>
<p>之所以存储物品向量而线上计算用户向量，是因为：</p>
<ul>
<li>每次召回只用到一个用户向量，而用到大量物品向量，线上计算物品向量代价过大。</li>
<li>用户特征变化较快，而物品特征相对稳定，离线存储用户向量不利于推荐效果。</li>
</ul>
<p>模型需要定期做更新，分为全量更新（天级别）和增量更新（实时）：</p>
<ul>
<li><strong>全量更新</strong>：每天做一次全量更新，训练整个模型，包括embedding和全连接层。
<ul>
<li>将昨天的数据random shuffle。</li>
<li>在昨天模型参数的基础上（不受增量更新影响），用昨天的数据训练1epoch。</li>
<li>训练完成后发布新的用户塔和物品向量。</li>
</ul></li>
<li><strong>增量更新</strong>：做online learning更新模型参数，只更新ID
Embedding。
<ul>
<li>用户兴趣会随时发生变化，因此需要及时更新模型。</li>
<li>实时收集线上数据，做流式处理，生成TFRecord文件。</li>
<li>对模型做online learning，增量更新ID Embedding参数。</li>
<li>发布用户ID Embedding（推荐时使用用户ID查询用户ID
Embedding），供用户塔在线上计算用户向量。</li>
</ul></li>
</ul>
<p>全量更新不受一天中不同时段用户使用习惯差异的影响，随机打乱数据训练效果更好。而增量更新可以动态捕捉用户的兴趣变化。因此二者需要结合使用。</p>
<h2 id="双塔模型自监督学习">双塔模型+自监督学习</h2>
<p>双塔模型学不好低曝光物品的向量表征：</p>
<ul>
<li>推荐系统的头部效应严重
<ul>
<li>少部分物品占据大部分点击</li>
<li>大部分物品点击次数不高</li>
</ul></li>
<li>高点击物品的表征学得好，长尾物品的表征学得不好。</li>
</ul>
<p>通过自监督学习做data
augmentation可以更好地学习长尾物品的向量表征：</p>
<ul>
<li>对每个物品做特征两类特征变换，并通过物品塔得到两个特征向量b'和b''。</li>
<li>对于同一个物品的两个特征向量<span
class="math inline">\(b_i&#39;和b_i&#39;&#39;\)</span>应使其相似度高。</li>
<li>对于不同物品的特征向量<span
class="math inline">\(b_i&#39;和b_j&#39;&#39;\)</span>应使其相似度低。</li>
</ul>
<h3 id="特征变换">特征变换</h3>
<p>对物品做特征变换时主要有几种方式：</p>
<ul>
<li><p><strong>Random
Mask</strong>：随机选取一些离散特征（比如类目），把它们遮住。例如：</p>
<ul>
<li>某物品的类目特征是u={数码，摄影}。</li>
<li>mask后的类目特征变成u'={default}。</li>
</ul></li>
<li><p><strong>Dropout</strong>：随机丢弃某个多值离散特征50%的值。</p>
<ul>
<li>多值离散特征：可以有多个取值的特征，例如类目。</li>
<li>例如某个物品的类目特征u={数码，摄影} dropout后变成了u'={美妆}。</li>
</ul></li>
<li><p><strong>互补特征(complementary)</strong>：</p>
<ul>
<li>假设物品一个有4种特征：{ID，类目，关键词，城市}。</li>
<li>随机分成两组：{ID，关键词}和{类目，城市}。</li>
<li>由{ID，default，关键词，default}计算得到物品表征1。</li>
<li>由{default，类目，default，城市}计算得到物品表征2。</li>
<li>应使物品表征1和物品表征2尽可能相似。</li>
</ul></li>
<li><p><strong>Mask一组关联的特征</strong></p>
<ul>
<li><p>离线计算特征两两之间的关联，用互信息（mutual information）衡量：
<span class="math display">\[
MI(u,v)=\sum_{u∈U}\sum_{v∈V}p(u,v)·log \frac{p(u,v)}{p(u)·p(v)}
\]</span>
其中p(u)表示某特征取值为u的概率，p(u,v)表示某两个特征分别取值u、v的概率。</p></li>
<li><p>设一共有k种特征，则两两之间的MI构成k×k矩阵。</p></li>
<li><p>随机选一个特征作为种子，mask种子及其相关的k/2种特征。</p></li>
<li><p>效果好但成本高。</p></li>
</ul></li>
</ul>
<h3 id="训练模型">训练模型</h3>
<p>首先对点击做随机抽样，得到n对用户-物品二元组，作为一个batch训练双塔。这种方式下热门物品抽到的概率更高。</p>
<p>接着从全体物品中<strong>均匀</strong>抽样，得到m个物品，作为一个batch，用来训练物品塔。这种方式下所有物品被抽到概率相同。对这些物品做两类特征变换，物品塔输出两组向量：<span
class="math inline">\(b_1&#39;,b_2&#39;,···,b_m&#39;和b_1&#39;&#39;,b_2&#39;&#39;,···,b_m&#39;&#39;\)</span>。</p>
<p>如图，取第一组中的向量<span
class="math inline">\(b_i&#39;\)</span>，分别与第二组中的向量取余弦，要使正样本的余弦值尽可能接近1，其余尽可能接近0。使用交叉熵损失函数。</p>
<figure>
<img src="训练模型.png" alt="训练模型" />
<figcaption aria-hidden="true">训练模型</figcaption>
</figure>
<p>如此得到了第i个物品的损失函数： <span class="math display">\[
L_{self}[i]=-log(\frac{exp(cos(b_i&#39;,b_i&#39;&#39;))}{\sum_{j=1}^{m}exp(cos(b_i&#39;,b_j&#39;&#39;))})
\]</span> 对m个物品的损失取平均作为自监督学习的损失： <span
class="math display">\[
loss_{self}=\frac{1}{m}\sum_{i=1}^{m}L_{self}[i]
\]</span>
双塔模型的损失和自监督学习的损失共同构成整个模型的损失函数，其中超参数α用来调节自监督的作用：
<span class="math display">\[
loss=\frac{1}{n}\sum_{i=1}^{n}L_{main}[i]+α·\frac{1}{m}\sum_{j=1}^{m}L_{self}[j]
\]</span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://rookiedb0901.github.io/2024/10/09/%E7%89%A9%E5%93%81%E5%86%B7%E5%90%AF%E5%8A%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="DB">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="深夜食堂">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 深夜食堂">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/10/09/%E7%89%A9%E5%93%81%E5%86%B7%E5%90%AF%E5%8A%A8/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-10-09 18:32:59" itemprop="dateCreated datePublished" datetime="2024-10-09T18:32:59+08:00">2024-10-09</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><strong>物品冷启动</strong>指的是如何对新发布的物品做分发。优化物品冷启动在UGC平台尤为重要，这是因为新物品数量巨大，内容质量良莠不齐，分发非常困难。</p>
<ul>
<li>新物品缺少与用户的交互，推荐难度大、效果差。</li>
<li>扶持新发布、低曝光的物品，可以增强作者发布意愿。</li>
</ul>
<p>优化冷启动的<strong>目标</strong>：</p>
<ul>
<li>精准推荐：克服冷启的困难，把新物品推荐给合适的用户，不引起用户反感。</li>
<li>激励发布：流量向低曝光新物品倾斜，激励发布。</li>
<li>挖掘高潜：通过初期小流量的试探，找到高质量的物品，给与流量倾斜。</li>
</ul>
<p>物品冷启动的<strong>指标</strong>：</p>
<ul>
<li>作者侧指标：发布渗透率、人均发布量
<ul>
<li>发布渗透率=当日发布人数/日活人数（发布一篇或以上的用户算一个发布人数）。</li>
<li>人均发布量=当日发布笔记数/日活人数。</li>
</ul></li>
<li>用户侧指标：
<ul>
<li>新笔记指标：点击率、交互率</li>
<li>大盘指标：消费时长、日活、月活</li>
</ul></li>
<li>内容侧指标（非必需）：高热物品占比</li>
</ul>
<h1 id="召回">召回</h1>
<p>冷启动召回的难点是缺少用户交互，还没学好物品 ID
embedding，导致双塔模型效果不好。而且缺少用户交互会导致 ItemCF
不适用。因此物品冷启动需要新的召回通道。</p>
<h2 id="改造双塔模型">改造双塔模型</h2>
<p>新物品缺少用户交互，还没学好物品 ID
embedding，如果要使用双塔模型需要解决如何确定物品ID的问题。有两种解决方案：</p>
<p><strong>新物品使用default embedding</strong>：</p>
<ul>
<li>物品塔做ID
embedding时，让所有新物品共享一个学出来的ID，而不是用自己真正的ID。</li>
<li>Default embedding：共享的ID对应的embedding向量。</li>
<li>到下次模型训练的时候，新物品才有自己的ID embedding向量。</li>
</ul>
<p><strong>利用相似物品的embedding向量</strong>：</p>
<ul>
<li><p>查找top
k内容最相似（根据标签、文字、图片等）的高曝物品。</p></li>
<li><p>把k个物品的向量取平均作为新物品的向量。</p></li>
</ul>
<p>可以设置<strong>多个召回池</strong>，如1小时新笔记、6小时内新笔记、24小时内新笔记、30天内新笔记，让新笔记有更多曝光机会。所有召回池共享一个双塔模型，因此不会增加训练代价。</p>
<h2 id="类目召回">类目召回</h2>
<p>系统维护从类目到笔记的索引，每个类目下的笔记列表<strong>根据时间倒排</strong>。做召回时，根据用户画像取回用户感兴趣的几个类目下的笔记列表，每个笔记列表取最新的k篇笔记做为召回结果。</p>
<p>同样的原理还可以实现基于关键词的召回。</p>
<p>这样的方法有以下<strong>缺点</strong>：</p>
<ul>
<li>只对刚刚发布的新笔记有效，发布几小时后就再没有机会被召回。</li>
<li>弱个性化，不够精准。</li>
</ul>
<h2 id="聚类召回">聚类召回</h2>
<p>聚类召回是基于物品内容的召回通道。它假设如果用户喜欢一个物品，那么用户会喜欢内容相似的其他物品。可以事先训练一个神经网络，基于物品的类目和内容，把物品映射到向量。对物品向量做聚类，划分为1000个cluster，记录每个cluster的中心方向（k-means聚类，用余弦相似度）。</p>
<p><strong>聚类索引</strong>：</p>
<ul>
<li>当一个新物品发布后，用神经网络把它映射到一个特征向量。</li>
<li>从1000个向量（对应1000个cluster）中找到最相似的向量，作为新物品的cluster。</li>
<li>索引：cluster→物品ID列表（按时间倒排）。</li>
</ul>
<p><strong>线上召回</strong>：</p>
<ul>
<li>给定用户ID，找到塔的last-n交互的物品列表，把这些物品作为种子物品。</li>
<li>每个种子物品映射到向量，寻找最相似的cluster。</li>
<li>从每个cluster的笔记列表中，取回最新的m篇笔记，最多取回mn个新物品。</li>
</ul>
<p>聚类召回同样只对<strong>新发布的笔记</strong>有效，但聚类召回的推荐比简单按类目推荐更加精准。</p>
<p>需要训练一个<strong>神经网络</strong>来将内容映射成向量。例如对于图文内容，可以用CNN处理图片信息，用BERT处理文字信息，分别生成一个向量并做拼接，再用一个全连接层映射成新的向量。具体训练如下：</p>
<ul>
<li>使用模型预测种子物品、正样本、负样本的向量<span
class="math inline">\(a,b^+,b^-\)</span>。</li>
<li>鼓励<span
class="math inline">\(cos(a,b^+)尽可能大，cos(a,b^-)\)</span>尽可能小。使用Triplet
hinge loss或Triplet logistic
loss，这两个损失函数见召回的双塔模型训练。</li>
</ul>
<p>正样本的选取可以采用人工标注的方式，但这样成本太高。</p>
<p>可以利用<strong>算法自动选取</strong>正样本：</p>
<ul>
<li>筛选条件：
<ul>
<li>只用高曝光物品作为二元组（因为有充足的用户交互信息）。</li>
<li>两个物品有相同的二级类目。</li>
</ul></li>
<li>用ItemCF的物品相似度选取正样本。</li>
</ul>
<p>负样本可以直接从全体物品中随机选取内容较丰富、质量高的物品。</p>
<h2 id="look-alike人群扩散">Look-Alike人群扩散</h2>
<p>Look-Alike是互联网广告常用的方法，也可以应用在推荐系统。Look-Alike
适用于<strong>发布一段时间、但是点击次数不高</strong>的物品。</p>
<p>Look-Alike主要思想如下：</p>
<ul>
<li>如果用户与物品发生点击、点赞等交互，则认为用户对物品可能感兴趣。</li>
<li>把有交互行为的用户作为新物品的种子用户。</li>
<li>用look-alike在种子用户的相似用户中扩散。
<ul>
<li>取种子用户向量（双塔模型生成）的平均作为物品向量表征（近线更新，即分钟级别的更新），存储在向量数据库。</li>
<li>做推荐时，拿用户向量对向量数据库做最近邻查找。</li>
</ul></li>
</ul>
<p>物品从发布到热门，主要的透出渠道会经历三个阶段：</p>
<ul>
<li>类目召回、聚类召回。它们是基于内容的召回通道，适用于<strong>刚刚发布</strong>的物品。</li>
<li>Look-Alike
召回。它适用于<strong>有点击，但是点击次数不高</strong>的物品。</li>
<li>双塔、ItemCF、Swing
等等。它们是基于用户行为的召回通道，适用于<strong>点击次数较高</strong>的物品。</li>
</ul>
<h1 id="流量调控">流量调控</h1>
<p><strong>流量调控</strong>是物品冷启动最重要的一环，直接影响作者发布指标。工业中往往要把<strong>流量向新物品倾斜</strong>，这是为了：</p>
<ul>
<li>提高作者创作积极性，提高发布渗透率、人均发布量。</li>
<li>让每个新物品都能获得足够曝光，以挖掘优质笔记，提高高热物品占比。</li>
</ul>
<p>流量调控的发展通常会经历这几个阶段：</p>
<ul>
<li>在推荐结果中强插新物品。</li>
<li>对新物品做<strong>提权（boost）</strong>：给新物品的分数乘以一个系数。
<ul>
<li>容易实现，投入产出比好。</li>
<li>曝光量对提权系数很敏感，很难精确控制曝光量。</li>
</ul></li>
<li>通过提权，对新物品做<strong>保量</strong>。
<ul>
<li>例如保证物品24小时内获得100次曝光。</li>
<li>在原有提权系数上，离保量目标越远乘以一个更大的系数。</li>
<li>提权系数=<span
class="math inline">\(f(发布时间/目标时间，已有曝光/目标曝光)\)</span>。</li>
<li>因为推荐链路、提权系数、线上环境等问题，保量成功率往往远低于100%。</li>
<li>不能直接给所有新物品一个很大的提权系数，否则会把物品推荐给不合适的受众。</li>
</ul></li>
<li>差异化保量。
<ul>
<li>结合<strong>内容质量</strong>以及作者<strong>历史作品质量</strong>给予物品额外的保量目标。</li>
</ul></li>
</ul>
<h1 id="ab测试">AB测试</h1>
<p>推荐系统常用的AB测试只考察<strong>用户侧消费指标</strong>（新笔记点击率、交互率，大盘指标），而冷启动的AB测试还需要额外考察<strong>作者侧发布指标</strong>（发布渗透率和人均发布量）。</p>
<h2 id="用户侧实验">用户侧实验</h2>
<p>推荐系统标准的AB测试中，将所有用户分为实验组和对照组，将所有物品按不同的策略向两组用户分别做推荐，看是否存在diff。</p>
<p>将这种方法应用到冷启动的AB测试，可能会导致<strong>推全后与AB测试存在差异</strong>。例如这样一个实验：</p>
<ul>
<li>限定：保量100次曝光。</li>
<li>假设：新物品曝光越多，用户使用APP时长越低。</li>
<li>新策略：把新物品排序时的权重增大两倍。</li>
<li>结果（只看用户消费指标）：
<ul>
<li>AB测试的diff是负数（实验组不如对照组）。</li>
<li>如果推全，diff会缩小。</li>
</ul></li>
</ul>
<p>这是因为对实验组的新物品做提权，则实验组会看到更多的新物品。而新物品做保量100次曝光是确定的，新物品在实验组取得更多曝光，就会在对照组取得更少曝光，从而使diff变大。实验组策略的变化会对对照组造成影响，所以实验会和推全结果存在差异。</p>
<h2 id="作者侧实验">作者侧实验</h2>
<p><strong>作者侧实验</strong>比用户侧实验更加复杂，已知的实验方案都存在缺陷。</p>
<p><strong>方案一</strong>：</p>
<ul>
<li>将<strong>新笔记的作者</strong>分为实验组和对照组，实验组使用新策略，对照组使用原策略，同时面向所有用户做推荐。</li>
<li>这种方案新物品之间会<strong>抢流量</strong>。例如新策略是把新物品的权重增大两倍，由于实验组和对照组的新物品存在竞争关系，<strong>实验组的策略会对对照组造成较大影响</strong>，实验组的物品会得到更多的曝光而对照组的物品曝光将大大减少。</li>
<li>此外，如果新笔记和老笔记存在竞争关系，那么AB测试时只有<strong>50%</strong>带策略的新笔记跟所有老笔记抢流量，而推全后则是<strong>100%</strong>，实验和推全结果会存在差异。</li>
</ul>
<p><strong>方案二</strong>：</p>
<ul>
<li><p>将<strong>用户</strong>也分为实验组和对照组，实验组的新物品只向实验组的用户推荐，对照组的新物品只向对照组的用户推荐。</p></li>
<li><p>这种方案两组新物品不会抢流量，实验结果更可信。但是，新物品仍然会和老物品抢流量，AB测试和推全结果有差异。并且，这会导致给用户推荐的<strong>内容池较少一半</strong>，影响用户体验，降低消费测指标，结果得不偿失。</p></li>
</ul>
<p><strong>方案三</strong></p>
<ul>
<li>将<strong>老笔记</strong>也分为实验组和对照组，实验组的新物品和老物品都只对实验组的用户做推荐，对照组亦然。</li>
<li>这种方案相当于把整个平台一分为二，实验结果更加准确。但会严重损害消费指标。</li>
</ul>
<p>设计冷启动AB测试方案时，需要考虑几个问题：</p>
<ul>
<li>实验组、对照组新物品会不会抢流量？</li>
<li>新物品、老物品怎么抢流量？</li>
<li>同时各类笔记、用户，会不会让内容池变小？</li>
<li>如果对新笔记做保量，会发生什么？</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://rookiedb0901.github.io/2024/10/09/%E7%89%A9%E5%93%81%E5%A4%9A%E6%A0%B7%E6%80%A7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="DB">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="深夜食堂">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 深夜食堂">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/10/09/%E7%89%A9%E5%93%81%E5%A4%9A%E6%A0%B7%E6%80%A7/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-10-09 18:32:59" itemprop="dateCreated datePublished" datetime="2024-10-09T18:32:59+08:00">2024-10-09</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>在做推荐时，除了考虑用户是否对物品感兴趣，还要考虑推荐物品的<strong>多样性</strong>。如果多样性做得好，可以显著提升推荐系统的核心业务指标。因此，在做完精排后，还需要结合物品的多样性指标进行<strong>重排</strong>，以选出最终的推荐物品。</p>
<p>物品相似性有两种度量方法：</p>
<ul>
<li>基于<strong>物品属性标签</strong>，例如根据一级类目、二级类目、品牌计算相似度：
<ul>
<li>物品i：美妆、彩妆、香奈儿</li>
<li>物品j：美妆、香水、香奈儿</li>
<li>相似度：<span
class="math inline">\(sim_1(i,j)=1,sim_2(i,j)=0,sim_3(i,j)=1\)</span></li>
<li>对三个相似度求加权和，权重由人为规定</li>
</ul></li>
<li>基于<strong>物品向量表征</strong>
<ul>
<li>使用基于内容的向量表征。利用nlp和cv算法提取物品特征。可以使用CLIP方法。</li>
<li>不使用双塔模型学到的物品向量，因为推荐系统头部现象很严重，曝光和点击都集中在少数物品，双塔模型学不好新物品和长尾物品的向量表征。</li>
</ul></li>
</ul>
<p>在推荐的链路上，在粗排和精排的<strong>后处理</strong>阶段，综合排序模型打分和多样性分数做选择。</p>
<ul>
<li>粗排和精排用多目标模型对物品做pointwise打分。</li>
<li>对于物品i，模型输出点击率、交互率的预估，融合成分数<span
class="math inline">\(reward_i\)</span>。</li>
<li>后处理阶段从n个物品选出k个，既要求总分高，也需要有多样性。</li>
</ul>
<h1 id="mmr">MMR</h1>
<p>Maximal Marginal Relevance
(MMR)来自于搜索算法，根据精排打分和物品相似度，从 n 个物品中选出 k
个价值高、且多样性好的物品。</p>
<p>MMR多样性算法的步骤如下：</p>
<ul>
<li><p>已选中的物品S初始化为空集，未选中的物品R初始化为全集{1,···,n}。</p></li>
<li><p>选择精排分数<span
class="math inline">\(reward_i\)</span>最高的物品，从集合R移到S。</p></li>
<li><p>做k-1轮循环：</p>
<ul>
<li><p>计算未选中物品集合R中所有物品的分数<span
class="math inline">\(\{MR_i\}_{i∈R}\)</span>。，其中： <span
class="math display">\[
MR_i=θ·reward_i-(1-θ)·\max_{j∈S}sim(i,j)
\]</span></p></li>
<li><p>选出分数最高的物品，将其从R移到S。</p></li>
</ul></li>
</ul>
<h1 id="dpp">DPP</h1>
<p>行列式点过程 (determinantal point process, DPP)
是一种经典的机器学习方法，是目前推荐系统重排多样性公认的最好方法。</p>
<h2 id="超平形体">超平形体</h2>
<p>一组向量<span
class="math inline">\(v_1,···,v_k∈R^d\)</span>可以确定一个<strong>k维超平行体</strong>：
<span class="math display">\[
P(v_1,···,v_k)=\{α_1v_1+···+α_kv_k|0≤α_1,···,α_k≤1\}
\]</span></p>
<ul>
<li><p>要求k≤d，比如d=3维空间中有k=2维平行四边形。</p></li>
<li><p>如果v_1,···,v_k线性相关，则体积vol(P)=0。</p></li>
</ul>
<p>我们可以借助k维超平行体的体积来<strong>衡量物品多样性</strong>：</p>
<ul>
<li>给定k个物品，把他们表征维单位向量<span
class="math inline">\(v_1,···,v_k∈R^d（d≥k）\)</span>。</li>
<li>用超平行体的体积衡量物品的多样性，体积介于0和1之间。</li>
<li>如果k个单位向量两两正交，则体积最大化，vol=1。</li>
</ul>
<p>关于k维超平行体的体积，有如下计算方法：</p>
<ul>
<li>将k个向量作为矩阵<span
class="math inline">\(V∈R^{d×k}\)</span>的列向量。</li>
<li>若d≥k，则行列式与体积满足：<span
class="math inline">\(det(V^TV)=vol(P(v_1,···,v_k))^2\)</span></li>
</ul>
<h2 id="行列式点过程dpp">行列式点过程DPP</h2>
<p>基于以上原理，DPP用行列式来衡量物品多样性，其目标函数可写为： <span
class="math display">\[
\mathop{\arg\max}_{S:|S|=k} \ \ log \ det(V_S^TV_S)
\]</span> Hulu的论文将DPP应用在推荐系统，使用物品相似度来调节物品得分：
<span class="math display">\[
\mathop{\arg\max}_{S:|S|=k} \ \ θ·(\sum_{j∈S}reward_j)+(1-θ)·log \
det(V_S^TV_S)
\]</span> 我们令k×k矩阵<span
class="math inline">\(A_S=V_S^TV_S\)</span>。设A为n×n矩阵，它的(i,j)元素为<span
class="math inline">\(a_{i,j}=v_i^Tv_j\)</span>。给定向量<span
class="math inline">\(v_1,···,v_n∈R^d\)</span>，需要<span
class="math inline">\(O(n^2d)\)</span>的时间计算A。<span
class="math inline">\(A_S\)</span>为A的一个子矩阵。如果i,j∈S,则<span
class="math inline">\(a_{ij}\)</span>是<span
class="math inline">\(A_s\)</span>的一个元素。那么，DPP得分可以写成：
<span class="math display">\[
\mathop{\arg\max}_{S:|S|=k} \ \ θ·(\sum_{j∈S}reward_j)+(1-θ)·log \
det(A_S)
\]</span>
DPP是个组合优化问题，要求从{1,···,n}中选出一个大小为k的子集S。通常用贪心算法来近似求解DPP。用S表示已选中的物品，R表示未选中的物品，则每一轮从R中选择一个物品i，满足：
<span class="math display">\[
\mathop{\arg\max}_{i∈R} \ \ θ·reward_i+(1-θ)·log \ det(A_{S∪\{i\}})
\]</span>
也就是说，我们所选择的i，既要价值尽可能高，又要使i加入后物品多样性尽可能高。</p>
<p>但是，要求解这个问题需要计算行列式很多次，而计算行列式需要矩阵分解，代价很大。</p>
<ul>
<li>对于单个i，计算<span
class="math inline">\(A_{S∪{i}}\)</span>的行列式需要<span
class="math inline">\(O(|S|^3)\)</span>时间。</li>
<li>对于所有的i∈R，计算行列式需要时间<span
class="math inline">\(O(|S|^3·|R|)\)</span>。</li>
<li>我们需要选出k个物品，因此复杂度为<span
class="math inline">\(O(|S|^3·|R|·k)=O(nk^4)\)</span>。</li>
<li>计算矩阵A的时间复杂度为<span
class="math inline">\(O(n^2d)\)</span>，因此总时间复杂度为<span
class="math inline">\(O(n^2d+nk^4)\)</span></li>
</ul>
<p>Hulu的论文设计了一种数值算法，仅需<span
class="math inline">\(O(n^2d+nk^2)\)</span>的时间就可以从n个物品挑选出k个物品：</p>
<ul>
<li>Cholesky分解<span
class="math inline">\(A_S=LL^T\)</span>，其中L是下三角矩阵。</li>
<li><span class="math inline">\(A_S\)</span>的行列式为<span
class="math inline">\(det(A_S)=det(L)^2=\prod_il^2_{ii}\)</span>。</li>
<li>已知<span
class="math inline">\(A_S=LL^T\)</span>，则可以快速求出所有<span
class="math inline">\(A_{S∪\{i\}}\)</span>的Cholesky分解，进而求出其行列式。</li>
</ul>
<h1 id="滑动窗口">滑动窗口</h1>
<p>在MMR中，我们每一轮都要选出一个物品i，使得： <span
class="math display">\[
\mathop{\arg\max}_{i∈R} \ \ θ·reward_i-(1-θ)·\max_{j∈S}sim(i,j)
\]</span>
这里的S指已选中物品的集合。当|S|很大时，很难找出物品i与S中的物品不相似了。即当|S|很大时，相似性sim(i,j)总是约等于1，导致MMR方法失效。</p>
<p>可以使用<strong>滑动窗口</strong>：设置一个滑动窗口W，比如最近选中的10个物品，用W代替MMR公式中的S：
<span class="math display">\[
\mathop{\arg\max}_{i∈R} \ \ θ·reward_i-(1-θ)·\max_{j∈W}sim(i,j)
\]</span>
这种方案下，只考虑最近选中的物品，要求距离较近的物品相似度低，而距离较远的物品相似度可以较高。这也符合实践的需要，当间隔较远时，物品相似度高并不会影响用户体验。</p>
<p>同样的，滑动窗口也可以应用到DPP，使用最近选中物品集合W代替S： <span
class="math display">\[
\mathop{\arg\max}_{i∈R} \ \ θ·reward_i+(1-θ)·log \ det(A_{W∪\{i\}})
\]</span></p>
<h1 id="业务规则约束">业务规则约束</h1>
<p>在实际业务中往往有很多规则应用在重排阶段，这些规则和MMR、DPP等算法结合来满足用户的多样性体验。以小红书为例，可能会出现以下规则：</p>
<ul>
<li>最多连续出现k篇某种笔记（如图文笔记、视频笔记）。</li>
<li>每k篇笔记最多出现1篇某种笔记（如运营推广笔记）。</li>
<li>前t篇笔记最多出现k篇某种笔记（小红书前4篇笔记为首屏，更容易被用户看到）。</li>
</ul>
<p>这些业务规则可以和MMR和DPP结合，每一轮都先用规则排除掉R中的部分物品，从剩下的物品中做选择。例如前k篇笔记都是图文笔记，那么下一篇笔记要把图文笔记都排除掉。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://rookiedb0901.github.io/2024/10/09/%E7%89%B9%E5%BE%81%E4%BA%A4%E5%8F%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="DB">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="深夜食堂">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 深夜食堂">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/10/09/%E7%89%B9%E5%BE%81%E4%BA%A4%E5%8F%89/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-10-09 18:32:59" itemprop="dateCreated datePublished" datetime="2024-10-09T18:32:59+08:00">2024-10-09</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="因式分解机fm">因式分解机FM</h1>
<p>线性模型对输入的特征取加权和，作为对目标的预估。如果先做特征交叉，再用线性模型，通常可以取得更好的效果。如果做二阶特征交叉，那么参数量为O(特征数量平方)，计算量大，而且容易造成过拟合。因式分解机（Factorized
Machine, FM）用低秩矩阵分解的方式降低参数量，加速计算。</p>
<p>若有x个特征，则线性模型可表示为： <span class="math display">\[
p = b+\sum_{i=1}^dw_ix_i
\]</span> 加入二阶交叉特征： <span class="math display">\[
p=b+\sum_{i=1}^dw_ix_i+\sum_{i=1}^d\sum_{j=i+1}^ju_{ij}x_ix_j
\]</span>
引入特征交叉后，模型表达能力更强，也能取得更好的效果。但同时，直接加入二阶交叉特征引入了O(d^2)级别的参数量，大大提高了计算量。</p>
<p>相比于直接训练一个d×d的矩阵作为交叉项的参数，因式分解机只训练一个d×k的矩阵V(k&lt;&lt;d)，将该矩阵第i列和第j列的内积作为交叉项<span
class="math inline">\(x_ix_j\)</span>的参数： <span
class="math display">\[
p=b+\sum_{i=1}^dw_ix_i+\sum_{i=1}^d\sum_{j=i+1}^jv_i^Tv_jx_ix_j
\]</span>
如此一来，参数量便大大减小。任何可以用线性模型（比如线性回归、逻辑回归）解决的问题，都可以用
FM 解决。但当前FM已经不常使用了。</p>
<h1 id="深度交叉网络dcn">深度交叉网络DCN</h1>
<p>深度交叉网络（Deep &amp; Cross Networks,
DCN）用来代替简单的全连接网络，可以用于召回双塔模型、粗排三塔模型、精排模型。DCN
由一个<strong>深度网络</strong>和一个<strong>交叉网络</strong>组成，交叉网络的基本组成单元是<strong>交叉层</strong>。</p>
<figure>
<img src="交叉层.png" alt="交叉层" />
<figcaption aria-hidden="true">交叉层</figcaption>
</figure>
<p>一个交叉层做了如下运算： <span class="math display">\[
x_{i+1}=x_0⊙(W_ix_i+b_i)+x_i
\]</span>
其中⊙为哈达玛积运算，即按位置相乘。一个交叉网络可以由多个交叉层叠加而成，类似ResNet。</p>
<p>深度交叉网络由一个深度网络和一个交叉网络组成：</p>
<figure>
<img src="深度交叉网络.png" alt="深度交叉网络" />
<figcaption aria-hidden="true">深度交叉网络</figcaption>
</figure>
<p>特征向量经过全连接网络和交叉网络后分别得到一个向量，两个向量合并再输入一个全连接层得到最终的输出。</p>
<p>深度交叉网络可以用来代替召回、排序模型中的神经网络，如双塔模型中的用户塔物品塔，且往往能取得更好的效果，得到工业界的广泛认可。</p>
<h1 id="lhuc网络结构">LHUC网络结构</h1>
<p>LHUC 的起源是语音识别，后来被应用到推荐系统，LHUC
可以用于<strong>精排</strong>。快手将其称为
PPNet，现在已经在业界广泛落地。</p>
<figure>
<img src="推荐系统中的LHUC模型.png" alt="推荐系统中的LHUC模型" />
<figcaption aria-hidden="true">推荐系统中的LHUC模型</figcaption>
</figure>
<p>LHUC的模型结构如上图所示。模型的输入为用户特征和物品特征，输出为一个向量。需要注意的是，图中的两个神经网络由多个全连接层和一个**2*sigmoid<strong>激活函数组成，将最终的结果数值大小限定到</strong>(0,2)<strong>范围内。这样可以在和其他向量做哈达玛积后，</strong>放大某些数值，缩小某些数值**，具体放大哪些缩小哪些则受用户特征影响。</p>
<p>将LHUC用于推荐系统，门控神经网络（2 x sigmoid）的梯度不要传递到用户ID
embedding特征，需要对其做 stop gradient。</p>
<p>LHUC模型原来用来做语音识别，输入分别为语音信号和说话者的特征，根据说话者的特征对语音信号的输出进行调节。</p>
<h1 id="fibinet">FiBiNet</h1>
<p>学习FiBiNet需要先了解SENet和Bilinear Cross。</p>
<h2 id="senet">SENet</h2>
<p>SENet
是计算机视觉中的一种技术，可以用在推荐系统中对特征做<strong>动态加权</strong>。</p>
<figure>
<img src="SENet.PNG" alt="SENet" />
<figcaption aria-hidden="true">SENet</figcaption>
</figure>
<p>假设有m个特征向量（这些向量的长度可不同），对这些向量做变换得到一个m维向量。将该m维向量作为权重对原来的特征向量做加权，得到新的特征向量。SENet的作用在于对离散特征做field-wise加权，放大或削弱某些特征的作用。比如用户ID
Embbedding是64维向量，则这64个元素算一个field，获得相同的权重。m个特征向量即m个fields。</p>
<h2 id="bilinear-cross">Bilinear Cross</h2>
<p>Bilinear
Cross用于做Field间特征交叉，即将两个fields做交叉得到新的特征。Field间特征交叉通常有以下几种方式。</p>
<ul>
<li><strong>内积</strong>：将两个特征向量做内积得到一个实数。如果有m个离散特征，就会得到m^2个实数。</li>
<li><strong>哈达玛积</strong>：将两个特征向量做哈达玛积得到一个向量。如果有m个离散特征，就会得到m^2个向量。内积和哈达玛积都要求向量维度相同。</li>
<li><strong>Bilinear Cross</strong>：
<ul>
<li>内积：<span
class="math inline">\(f_{ij}=x_i^TW_{ij}x_j\)</span>。如果有m个fields则需要<span
class="math inline">\(m^2/2\)</span>个参数矩阵，参数量非常大，因此只能人工指定一些重要的、相关的特征做交叉。</li>
<li>哈达玛积：<span
class="math inline">\(f_{ij}=x_i⊙(W_{ij}x_j)\)</span>。如果有m个fields则会产生m^2个向量，往往也是人工指定一些特征做交叉。</li>
</ul></li>
</ul>
<h2 id="fibinet-1">FiBiNet</h2>
<p>FiBiNet结合了以上两种技术。</p>
<figure>
<img src="FiBiNet.PNG" alt="FiBiNet" />
<figcaption aria-hidden="true">FiBiNet</figcaption>
</figure>
<p>传统的模型中，离散特征经过embedding后直接和连续特征拼起来作为神经网络的输入。FiBi加入了SENet和Bilinear，具体如上图，其中Bilinear输出的是几个交叉后的向量，再将这些向量做拼接得到一个高维向量。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://rookiedb0901.github.io/2024/10/09/%E6%8E%92%E5%BA%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="DB">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="深夜食堂">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 深夜食堂">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/10/09/%E6%8E%92%E5%BA%8F/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-10-09 18:32:59" itemprop="dateCreated datePublished" datetime="2024-10-09T18:32:59+08:00">2024-10-09</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>推荐系统需要经过召回、粗排、精排、重排等过程才能将物品推荐给用户。其中粗排和精排使用类似的模型，粗排使用更小的模型对召回的结果进行打分，筛选出更少的笔记后交给更大更准确的精排模型打分，以此提高效率。</p>
<p>以小红书为例，排序中常用的指标有：点击率（点击次数/曝光次数）、点赞率（点赞次数/点击次数）、收藏率（收藏次数/点击次数）、转发率（转发次数/点击次数）等。</p>
<p>排序模型预估以上多种分数，并融合这些分数，根据融合的分数做排序、截断。</p>
<h1 id="多目标模型">多目标模型</h1>
<p>多目标模型在工业界被广泛运用。</p>
<figure>
<img src="多目标模型.png" alt="多目标模型" />
<figcaption aria-hidden="true">多目标模型</figcaption>
</figure>
<p>如图，各类特征向量concatenation后一起输入神经网络，神经网络输出的向量再分别输入四个神经网络，即得到点击率、点赞率、收藏率、转发率的预测。由于使用了sigmoid激活函数，四个预估值都介于0，1之间。使用用户的真实行为作为标签，例如一个用户点击并转发了物品，但未点赞、收藏，则四个标签为（1，0，0，1）。可以把四种行为分别作为一个<strong>二元分类任务</strong>，使用交叉熵作为损失函数，则总损失函数可表示为：
<span class="math display">\[
\sum_{i=1}^{4}α_iCrossEntropy(y_i,p_i)
\]</span> 其中<span
class="math inline">\(α_i\)</span>为人为设置的权重。</p>
<p>训练中往往存在类别不平衡的问题，例如每100次曝光，可能只有10次点击，而有90次未点击。因此需要对负样本做<strong>降采样</strong>，只保留一小部分负样本，让正负样本数量平衡。</p>
<p>但是，经过降采样后，负样本变少，又会导致预估点击率大于真实点击率：</p>
<ul>
<li><p>正样本、负样本数量为<span
class="math inline">\(n_+和n_-\)</span>，负样本采样率为α，则实际使用<span
class="math inline">\(α·n_-\)</span>个负样本。</p></li>
<li><p>真实点击率：<span
class="math inline">\(p_{true}=\frac{n_+}{n_++n_-}\)</span>（期望）</p></li>
<li><p>预估点击率：<span
class="math inline">\(p_{pred}=\frac{n_+}{n_++α·n_-}\)</span>（期望）</p></li>
</ul>
<p>因此需要<strong>预估值校准</strong>： <span class="math display">\[
p_{true}=\frac{α·p_{pred}}{(1-p_{pred})+α·p_{pred}}
\]</span></p>
<h1 id="mmoe">MMoE</h1>
<p>MMoE全称Multi-gate Mixture-of-Experts，是对多目标模型的改进。</p>
<figure>
<img src="MMoE.png" alt="MMoE" />
<figcaption aria-hidden="true">MMoE</figcaption>
</figure>
<ul>
<li>将特征向量输入n个神经网络（称为“专家”，通常为4或8个，图中为3个），生成n个向量。</li>
<li>使用神经网络生成一个n维向量，作为n个“专家”生成向量的权重。</li>
<li>对n个向量做加权平均，再经过一个神经网络生产点击率的预估值。</li>
<li>同理，如果需要预估点赞率等其他指标，则添加更多生成权重的神经网络。</li>
</ul>
<p>在实践中，MMoE常常会产生<strong>极化现象</strong>：</p>
<ul>
<li>softmax输出值一个接近1，其余接近0，导致只有一个“专家”起作用，退化成简单的多目标模型。</li>
<li>在训练时，可以对softmax的输出使用dropout。
<ul>
<li>softmax输出的n个数值以10%的概率被mask，即每个专家都有10%的概率被丢弃。</li>
<li>这使得模型不会过分依赖某个“专家”，否则当其权重被mask时，将导致严重偏差。</li>
</ul></li>
</ul>
<p>MMoE的使用未必会带来提升，具体应根据实践结果选择。</p>
<h1 id="融合预估分数">融合预估分数</h1>
<p>前面讲到多目标模型可以对点击率、点赞率等指标做出预估，而要对物品进行排序，需要将这些指标进行融合得到最终的预估分数。以下是几种融分公式。</p>
<ul>
<li><p>简单加权和：将预估的点击率、点赞率、收藏率等指标直接做加权平均。
<span class="math display">\[
p_{click}+w_1·p_{like}+w_2·p_{collect}+···
\]</span></p></li>
<li><p>点击率乘以其他项的加权和： <span class="math display">\[
p_{click}·(1+w_1·p_{like}+w_2·p_{collect}+···)
\]</span>
该公式有其实际意义，例如点击率×点赞率=点赞次数/曝光次数。</p></li>
<li><p>海外某短视频APP融分公式： <span class="math display">\[
(1+w_1·p_{time})^{a_1}·(1+w_2·p_{like})^{α_2} ···
\]</span> 其中w和α都是超参数。</p></li>
<li><p>国内某短视频APP融分公式：</p>
<ul>
<li><p>根据预估时长<span
class="math inline">\(p_{time}\)</span>，对n篇候选视频做排序</p></li>
<li><p>如果某视频排名第<span
class="math inline">\(r_{time}\)</span>，则它得分<span
class="math inline">\(\frac{1}{r_{time}^α+β}\)</span></p></li>
<li><p>对点击、点赞、转发、评论等预估分数做类似处理。</p></li>
<li><p>最终融合分数： <span class="math display">\[
\frac{w_1}{r_{time}^α+β_1}+\frac{w_2}{r_{click}^α+β_2}+\frac{w_3}{r_{like}^α+β_3}+···
\]</span></p></li>
</ul></li>
<li><p>国内某电商的融分公式：</p>
<ul>
<li>电视转化流程：曝光→点击→加购→付款</li>
<li>最终融合分数：<span
class="math inline">\(p_{click}^{α_1}×p_{cart}^{α_2}×p_{pay}^{α_3}×price^{α_4}\)</span></li>
</ul></li>
</ul>
<h1 id="视频播放建模">视频播放建模</h1>
<p>除了点击、点赞、收藏、转发、评论等指标外，视频排序还需考虑<strong>播放时长</strong>和<strong>完播</strong>。</p>
<p><strong>播放时长</strong>建模：</p>
<ul>
<li><p>将多目标模型最后一个全连接层的输出记作z。设p=sigmoid(z)。</p></li>
<li><p>实际观测的播放时长记作t。</p></li>
<li><p>做训练：用p拟合y=t/(1+t)，最小化交叉熵损失 <span
class="math display">\[
loss=-(\frac{t}{1+t}·logp+\frac{1}{1+t}·log(1-p))
\]</span></p></li>
<li><p>做推理：把exp(z)作为播放时长的预估。</p></li>
</ul>
<p><strong>视频完播</strong>建模：</p>
<ul>
<li><p>回归方法</p>
<ul>
<li><p>例：视频长度10分钟，实际播放4分钟，则实际播放率为y=0.4。</p></li>
<li><p>让预估播放率p拟合y： <span class="math display">\[
loss=y·logp+(1-y)·log(1-p)
\]</span></p></li>
</ul></li>
<li><p>二元分类方法</p>
<ul>
<li>定义完播指标，比如完播80%，则10分钟的视频播放&gt;8分值作为正样本，播放&lt;8分钟作为负样本。</li>
<li>做二元分类训练模型。</li>
</ul></li>
<li><p>不能直接把预估的完播率用到融分公式，否则对长视频不公平。可以用函数f(视频时长)拟合完播率关于视频时长的函数。线上预估完播率，然后做调整：
<span class="math display">\[
p_{finish}=\frac{预估完播率}{f(视频长度)}
\]</span> 再把调整后的结果作为融分公式的一项。</p></li>
</ul>
<h1 id="排序模型的特征">排序模型的特征</h1>
<p>前面谈及排序模型时，使用到了用户特征、物品特征、统计特征、场景特征作为模型的输入。下面介绍这些特征。</p>
<p><strong>用户画像</strong></p>
<ul>
<li>用户ID（在召回、排序中做embedding）</li>
<li>人口统计学属性：性别、年龄等。</li>
<li>账号信息：新老、活跃度······</li>
<li>感兴趣的类目、关键词、品牌。</li>
</ul>
<p><strong>物品画像</strong></p>
<ul>
<li>物品ID（在召回、排序中做embedding）</li>
<li>发布时间</li>
<li>GeoHash（经纬度编码）、所在城市。</li>
<li>标题、类目、关键词、品牌······</li>
<li>字数、图片数、视频清晰度、标签数······</li>
</ul>
<p><strong>用户统计特征</strong></p>
<ul>
<li>用户最近30天（7天、1天、1小时）的曝光数、点击数、点赞数、收藏数······</li>
<li>按物品类目分桶</li>
</ul>
<p><strong>物品统计特征</strong></p>
<ul>
<li>笔记最近的曝光数、点击数、点赞数、收藏数······</li>
<li>按照用户性别、用户年龄、用户地域等分桶</li>
<li>作者特征（作品数、粉丝数、消费指标等）</li>
</ul>
<p><strong>场景特征</strong></p>
<ul>
<li>用户定位GeoHash（经纬度编码）、城市。</li>
<li>当前时刻（分段，做embedding）。</li>
<li>是否周末、节假日。</li>
<li>手机品牌、手机型号、操作系统等。</li>
</ul>
<p>要将以上特征转化为特征向量，需要经过<strong>特征处理</strong>：</p>
<ul>
<li>离散特征：做embedding。
<ul>
<li>用户ID、物品ID、作者ID</li>
<li>类目、关键词、城市、手机品牌等</li>
</ul></li>
<li>连续特征：做分桶，变成离散特征。
<ul>
<li>年龄、笔记字数、视频长度</li>
</ul></li>
<li>连续特征：其他变换。
<ul>
<li>曝光数、点击数、点赞数等数值做log(1+x)（减小极端值带来的影响）</li>
<li>转化为点击率、点赞率等值，并做平滑</li>
</ul></li>
</ul>
<p>很多特征无法覆盖100%的样本，例如很多用户未填写年龄、未开启GPS权限。因此做特征工程时，需要想办法提高<strong>特征覆盖率</strong>，以使模型更准确。</p>
<h1 id="粗排模型">粗排模型</h1>
<p>推荐需要经过粗排、精排两个过程，如果粗排对几千个物品打分，那精排可能只对几百个物品打分。因此，粗排单次推理代价必须小，而可以适当牺牲预估的准确度。</p>
<p>前面讲到的排序模型采用的时<strong>前期融合</strong>的方式，即先对所有特征做concatenation，再输入神经网络。这样的方式线上推理代价大，如果有n个候选物品则要做n次推理。这样的模型往往用于精排。</p>
<p>而用来做召回的双塔模型使用的是<strong>后期融合</strong>的方式，先用用户特征和物品特征分别计算出特征向量，再求余弦相似度。这种方式线上对于每个用户只需要做一次用户特征的推理，代价很小，但预估准确性也更低。</p>
<p>小红书在粗排阶段使用了介于二者之间的三塔模型。</p>
<figure>
<img src="三塔模型.png" alt="三塔模型" />
<figcaption aria-hidden="true">三塔模型</figcaption>
</figure>
<ul>
<li>用户塔很大，因为对于每个用户用户塔只需要做一次推理。</li>
<li>物品塔需要对新物品做推理，但物品塔的输出向量会被缓存，因此可以避免绝大多数推理。</li>
<li>对每个物品交叉塔都需要做推理，而且因为统计特征会随时变化，交叉塔的输出向量不能缓存。因此交叉塔会做得足够小（往往只有一层）。</li>
<li>模型上层同样对每个物品都要做一次推理，因此该模型的推理大部分计算量在模型上层。</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://rookiedb0901.github.io/2024/04/20/GPT%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="DB">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="深夜食堂">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 深夜食堂">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/04/20/GPT%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" class="post-title-link" itemprop="url">GPT系列论文笔记</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-04-20 00:00:00" itemprop="dateCreated datePublished" datetime="2024-04-20T00:00:00+08:00">2024-04-20</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="gpt">GPT</h1>
<p>论文首先指出一个问题：NLP领域中，有标号的数据量太少，难以训练出有效的模型。</p>
<p>然后给出解决思路：先在没有标号的数据上训练一个预训练模型，再在子任务上用有标号的数据微调。</p>
<p>难点：</p>
<ul>
<li><p>目标函数如何选取：自监督</p></li>
<li><p>如何找到一种能有效应用于不同子任务的表示</p></li>
</ul>
<h2 id="预训练">预训练</h2>
<p>用窗口内的k个词元去预测下一个词元，要使模型输出与原文章相同的概率最大，即最大化以下似然函数：
<span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.697ex;" xmlns="http://www.w3.org/2000/svg" width="36.504ex" height="4.847ex" role="img" focusable="false" viewBox="0 -950 16134.8 2142.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mn" transform="translate(714,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(1117.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1506.6,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(2078.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(2745.3,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="munder" transform="translate(3801.1,0)"><g data-mml-node="mo"><path data-c="2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path></g><g data-mml-node="TeXAtom" transform="translate(600,-1084.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mi" transform="translate(5411.8,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(5709.8,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(6194.8,0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mi" transform="translate(6671.8,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(7422.8,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(7811.8,0)"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(8710.7,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="msub" transform="translate(8988.7,0)"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="TeXAtom" transform="translate(605,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(1123,0)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g><g data-mml-node="mo" transform="translate(10806.2,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(11250.9,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(11695.5,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(12140.2,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(12584.9,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(13029.5,0)"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="TeXAtom" transform="translate(605,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mo" transform="translate(14832.2,0)"><path data-c="3B" d="M78 370Q78 394 95 412T138 430Q162 430 180 414T199 371Q199 346 182 328T139 310T96 327T78 370ZM78 60Q78 85 94 103T137 121Q202 121 202 8Q202 -44 183 -94T144 -169T118 -194Q115 -194 106 -186T95 -174Q94 -171 107 -155T137 -107T160 -38Q161 -32 162 -22T165 -4T165 4Q165 5 161 4T142 0Q110 0 94 18T78 60Z"></path></g><g data-mml-node="mi" transform="translate(15276.8,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(15745.8,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span> 其中θ是基于transformer解码器的模型：</p>
<figure>
<img src="GPT模型图.jpg" alt="GPT模型图">
<figcaption aria-hidden="true">GPT模型图</figcaption>
</figure>
<p>与bert不同之处在于，GPT的注意力层带有掩码，训练时是用前k个词元预测下一个词元，而BERT模型的注意力层没有掩码，预测一个词元时能看到上下文信息。</p>
<h2 id="微调">微调</h2>
<p>给定序列x_1, x2, ... ,
x_m，对应标签为y，也就是我们要根据序列x去预测y的概率。我们把序列放入GPT模型中，拿到x_m对应的输出（x_m一般是构造序列时添加的Extract词元），乘以一个输出层并做softmax即得到y的概率，即：
<span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.667ex;" xmlns="http://www.w3.org/2000/svg" width="34.872ex" height="2.667ex" role="img" focusable="false" viewBox="0 -883.9 15413.6 1178.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1630,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="msup" transform="translate(1908,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(605,413) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(2916.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(3361.2,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(3805.9,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(4250.6,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(4695.2,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msup" transform="translate(5139.9,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(605,413) scale(0.707)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(6415.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(7082.5,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(8138.3,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(8607.3,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(9092.3,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mi" transform="translate(9642.3,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(10003.3,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(10881.3,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(11410.3,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(11982.3,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msubsup" transform="translate(12371.3,0)"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mi" transform="translate(609,413) scale(0.707)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(609,-265.5) scale(0.707)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g><g data-mml-node="msub" transform="translate(13651.1,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(977,-150) scale(0.707)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(15024.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span>
h的下标l表示是第l层，即最后一层的输出，上标m表示是x_m对应的输出。</p>
<p>目标函数： <span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -3.222ex;" xmlns="http://www.w3.org/2000/svg" width="31.037ex" height="5.371ex" role="img" focusable="false" viewBox="0 -950 13718.2 2374.1"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mn" transform="translate(714,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(1117.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1506.6,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="mo" transform="translate(2266.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(2933.3,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="munder" transform="translate(3989.1,0)"><g data-mml-node="mo" transform="translate(26.8,0)"><path data-c="2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path></g><g data-mml-node="TeXAtom" transform="translate(0,-1147.3) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(961,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(1239,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1729,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g><g data-mml-node="mi" transform="translate(5653.4,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(5951.4,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(6436.4,0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mi" transform="translate(6913.4,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(7664.4,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(8053.4,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(8543.4,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="msup" transform="translate(8821.4,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(605,413) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(9830,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(10274.6,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(10719.3,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(11164,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(11608.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msup" transform="translate(12053.3,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(605,413) scale(0.707)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(13329.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span> 微调时，同时使用了两个目标函数：</p>
<ul>
<li>在序列中预测下一个词</li>
<li>用完整的序列预测标号</li>
</ul>
<p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="27.263ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 12050.1 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mn" transform="translate(714,-150) scale(0.707)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g></g><g data-mml-node="mo" transform="translate(1117.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1506.6,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="mo" transform="translate(2266.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(2933.3,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msub" transform="translate(3989.1,0)"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mn" transform="translate(714,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(5106.7,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(5495.7,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="mo" transform="translate(6255.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(6866.9,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(7867.1,0)"><path data-c="1D706" d="M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z"></path></g><g data-mml-node="mo" transform="translate(8672.3,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path></g><g data-mml-node="msub" transform="translate(9394.6,0)"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mn" transform="translate(714,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(10512.1,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(10901.1,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="mo" transform="translate(11661.1,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span></p>
<p>论文中给出了四类常见应用场景的微调。</p>
<figure>
<img src="GPT微调任务.jpg" alt="GPT微调任务">
<figcaption aria-hidden="true">GPT微调任务</figcaption>
</figure>
<h3 id="classification">Classification</h3>
<p>在文本前添加Start词元，在文本后添加Extract词元，用Extract词元对应的输出向量过一个全连接层，例如共有10种分类，则全连接层输出大小为10。</p>
<h3 id="entailment">Entailment</h3>
<p>给出两段文本，做一个三分类问题（支持/反对/既不支持也不分对）</p>
<p>将两段文本串成一个序列输入模型。</p>
<h3 id="similarity">Similarity</h3>
<p>判断两段文本是否相似，因为GPT具有先后顺序，而相似是相互的，因此需要构造两个序列。两个序列的输出相加进入全连接层。</p>
<h3 id="multiple-choice">Multiple Choice</h3>
<p>做多选题，需要将每个选项分别与题干构造序列，Linear层的输出大小为1，表示该答案正确的置信度，选择置信度最大的序列。</p>
<h2 id="实现细节">实现细节</h2>
<ul>
<li>使用BPE的词元化方式，字典大小为4000</li>
<li>n_model = 768，layer = 12, n_heads = 12</li>
<li>采用可学习的位置编码，位置编码长度为3072</li>
<li>激活函数为GLUE</li>
</ul>
<p>与bert的差异：</p>
<ul>
<li>GPT预训练直接使用自然文本，没有使用[CLS]、[SEP]等词元</li>
<li>bert能捕捉上下文信息，而GPT只能单向捕捉信息</li>
<li>GPT采用BPE，bert采用WordPiece</li>
<li>bert增加了段编码，而GPT只有位置编码</li>
</ul>
<p>总结而言，GPT的思路是用大量自然文本做预训练，再用带标号的文本针对下游任务做微调，解决的是分类任务。这篇文章诞生于bert之前，二者的区别在于GPT训练时无法看到后面的数据，而bert可以看到上下文。</p>
<h1 id="gpt2">GPT2</h1>
<p>GPT2训练文本达到百万级，参数量达到15亿</p>
<p>文章提到，NLP传统训练方式是用一个数据集训练一个任务，进而引出多任务学习（Multitask
Learning），即只用一个数据集，但构造多个损失函数来达到能在多个任务使用的效果。这种方式虽然很早就提出了，但当时却不是很流行。当时主流的预训练+微调的模式仍需要针对下游任务用有标号的数据进行微调。</p>
<p>GPT2强调了zero-shot的设定，即只需要训练一个模型，不做微调就可以直接应用于各个下游任务。</p>
<p>模型直接由自然文本训练，由于没有微调环节，因此下游任务的输入必须与预训练的文本一样，而不能添加没有见过的符号（如Start、Extract）。</p>
<p>论文使用了prompt的方法（但作者并没有直接提出这个概念），例如要让模型做机器翻译任务，可以构造输入：</p>
<ul>
<li>translate to french, english text, french text</li>
</ul>
<p>GPT2的训练采用的是来自Reddit的有一定质量的文本数据（只爬取至少有三个karma的），一共40GB文本。</p>
<h2 id="实现细节-1">实现细节</h2>
<ul>
<li>GPT2采用pre-norm，即将layer
normalization放到每个sub-block之前，并在最后一个self-attention后再增加一个layer
normalization。</li>
</ul>
<p>总结而言，GPT2相较于GPT除了规模的提升，更重要的是GPT2能直接运用于下游任务。虽然GPT2在很多任务上和SOTA仍有差距，但其具有强大的通用性，并且论证了模型性能还将随着规模提升。</p>
<h1 id="gpt3">GPT3</h1>
<p>GPT2思路极具新意，但在实际任务中的表现却很一般。在zero-shot表现不佳的情况下，GPT3采用了few-shot，即将模型应用于不同任务时，给出几个样例供模型参考。</p>
<p>GPT3模型有175B的参数。</p>
<p>论文分别用few-shot、one-shot、zero-shot对不同规模的模型进行评估，结果显示当模型规模达到百亿以上时，效果有了显著的提升。</p>
<p>fine-tuning、zero-shot、one-shot、few-shot的区别：</p>
<ul>
<li>fine-tuning：用具有一定规模的样本微调模型，会改变模型参数</li>
<li>zero-shot：只给出任务描述，要求模型能预测出答案。例如：“Translate
English to Chinese: cheese =&gt;
”要求模型能回答出cheese对应的中文。</li>
<li>one-shot：除了任务描述外，还给出一个样例供模型参考，例如：“Translate
English to Chinese: cheese =&gt; 奶酪, biscuit =&gt;
”要求模型能回答出biscuit对应的中文。</li>
<li>few-shot：与one-shot类似，给出多个样例。</li>
</ul>
<p>作者提出了“In-context
learning”的概念，即样本不用来训练模型的参数，而是作为样例和问题一起提供给模型，比如few-shot。</p>
<p>GPT2为了提高数据质量，采用了来自reddit的数据，但是GPT3需要更大量级的数据，因此只能继续采用Common
Crawl数据集。Common Crawl数据较脏，因此作者对其进行了过滤：</p>
<ul>
<li>将Common
Crawl的数据作为低质量的样本（认为该数据集中大部分样本质量都较低），reddit上karma大于3的帖子作为高质量样本，训练一个二分类器，然后对Common
Crawl的数据进行分类，过滤掉低质量的数据。</li>
<li>去除掉数据集中的重复文章。（使用LSH算法检索相似文章）</li>
<li>加入一些其他的高质量数据集。</li>
</ul>
<h2 id="实现细节-2">实现细节</h2>
<ul>
<li>GPT3使用了稀疏的自注意力（locally banded sparse attention）。sparse
attention计算注意力时，只关注距离不超过K以及距离为K, 2K, 3K,
...的token，其余token注意力都设为0。</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://rookiedb0901.github.io/2024/04/12/InstrcutGPT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="DB">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="深夜食堂">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 深夜食堂">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/04/12/InstrcutGPT/" class="post-title-link" itemprop="url">InstructGPT论文笔记</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-04-12 00:00:00" itemprop="dateCreated datePublished" datetime="2024-04-12T00:00:00+08:00">2024-04-12</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="问题">问题：</h1>
<h1 id="目标">目标：</h1>
<h1 id="方法">方法：</h1>
<ul>
<li><p>人为制作数据集（问题+回答），对模型做SFT（有监督微调），但是人工写回答成本太高</p></li>
<li><p>用模型生成9个答案，并人工对这些答案进行排序，使用排序数据来训练奖励模型（6B）</p></li>
</ul>
<p>奖励模型会给好的回答打高分，给差的回答打低分</p>
<p>模型输出后过一个输出大小为1的线性层，获得分数</p>
<p>奖励模型的损失函数： <span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.475ex;" xmlns="http://www.w3.org/2000/svg" width="55.416ex" height="5.511ex" role="img" focusable="false" viewBox="0 -1342 24493.7 2435.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(298,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(783,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(1252,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mo" transform="translate(1721,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2110,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(2579,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3245.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(4301.6,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mfrac" transform="translate(5079.6,0)"><g data-mml-node="mn" transform="translate(683.3,676)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="msubsup" transform="translate(220,-784.5)"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="mn" transform="translate(845.3,353.6) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mi" transform="translate(748,-309.4) scale(0.707)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g><rect width="1626.6" height="60" x="120" y="220"></rect></g><g data-mml-node="msub" transform="translate(6946.2,0)"><g data-mml-node="mi"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g><g data-mml-node="TeXAtom" transform="translate(771,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(572,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(850,0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g></g><g data-mml-node="mo" transform="translate(1929.3,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(2207.3,0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mn" transform="translate(523,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g><g data-mml-node="mo" transform="translate(10260.9,0)"><path data-c="223C" d="M55 166Q55 241 101 304T222 367Q260 367 296 349T362 304T421 252T484 208T554 189Q616 189 655 236T694 338Q694 350 698 358T708 367Q722 367 722 334Q722 260 677 197T562 134H554Q517 134 481 152T414 196T355 248T292 293T223 311Q179 311 145 286Q109 257 96 218T80 156T69 133Q55 133 55 166Z"></path></g><g data-mml-node="mi" transform="translate(11316.7,0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g><g data-mml-node="mo" transform="translate(12144.7,0)"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="mi" transform="translate(12422.7,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(12720.7,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(13205.7,0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mo" transform="translate(13682.7,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(14071.7,0)"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></g><g data-mml-node="mo" transform="translate(14642.7,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(15031.7,0)"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(484,-150) scale(0.707)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g><g data-mml-node="mo" transform="translate(15897.3,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(16286.3,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(16858.3,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(17303,0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g></g><g data-mml-node="mo" transform="translate(18382.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(18993.5,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msub" transform="translate(19993.7,0)"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(484,-150) scale(0.707)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g><g data-mml-node="mo" transform="translate(20859.4,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(21248.4,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(21820.4,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(22265,0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g><g data-mml-node="mo" transform="translate(23048.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(23437.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(23826.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(24215.7,0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g></g></g></svg></mjx-container></span>
y_w和y_l是K个回答中的一对，其中y_w的排序比y_l高，分别算出两个回答的奖励分数，σ是sigmoid函数，最小化loss即最大化y_w和y_l的奖励分数差。</p>
<ul>
<li>强化学习RL</li>
</ul>
<p>使用PPO算法</p>
<p>使用SFT模型的参数初始化RL模型。</p>
<p>每轮用RL模型生成新的回答，并计算奖励分数（目标是最大化奖励分数），调整模型，再用新的模型生成回答，如此循环。（与传统方法不同在于，传统训练过程中数据标签不会发生改变，而这里y是不断调整的）</p>
<h1 id="数据来源">数据来源</h1>
<p>首先由人工写一些问答数据，用这些数据训练模型，然后将模型投入试用，并收集用户问题进行再次训练</p>
<ul>
<li>数据集分为三份：</li>
</ul>
<p>①SFT dataset（13K）：人工标注回答</p>
<p>②RM dataset（33K）：模型生成多个回答并人工排序</p>
<p>③PPO dataset（31K）：使用Reward Model生成回答</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://rookiedb0901.github.io/2024/04/09/BART%E5%92%8CT5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="DB">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="深夜食堂">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 深夜食堂">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/04/09/BART%E5%92%8CT5/" class="post-title-link" itemprop="url">BART和T5</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-04-09 00:00:00" itemprop="dateCreated datePublished" datetime="2024-04-09T00:00:00+08:00">2024-04-09</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>自回归(autoregressive)语言模型，如GPT，采用从左向右单向解码的方式，适用于自然语言生成（NLG）任务。非自回归(non-autoregressive)语言模型，如BERT，每个时刻的输出都可以充分利用双向信息，适用于自然语言理解（NLU）任务，但是在NLG上表现不佳。BERT采用transformer的编码器结构，GPT采用transformer的解码器结构，而BART和T5都采用了transformer的原始结构。</p>
<h1 id="预训练">预训练</h1>
<p>BART的预训练任务是带噪声的输入还原。BART共采用了两个预训练任务。</p>
<ul>
<li>Text
Infilling。mask文本中30%的字符，每处mask掉span长度的字符，span长度服从λ
=
3的泊松分布。例如对于序列ABCDE，添加噪声后可能变成A_B_E，其中AB之间插入了一个span长度为0的mask，CD也替换成了mask。将加噪后的文本作为解码器输入，要求模型还原出文本。</li>
<li>Sentence
Permutation。根据标点符号将句子顺序打乱，并要求模型将句子顺序复原。</li>
</ul>
<p>除此之外作者还尝试了Token Masking、Token Deletion、Document
Rotation等方法。经过对比实验Text
Infilling更有助于模型效果提升，而Sentence
permutation虽然提升不大，但作者假设模型规模提升后这个任务会有用。</p>
<figure>
<img src="BART.png" alt="BART">
<figcaption aria-hidden="true">BART</figcaption>
</figure>
<p>T5使用两种任务，分为无监督和有监督。其中无监督任务也是Span级别的mask，不过输出不需要还原整句，只需要输出mask掉的tokens就可以，总共mask15%字符。有监督任务提升不大，这里不展开说明。</p>
<h1 id="微调">微调</h1>
<p>BART的微调方式如下图：</p>
<ul>
<li><p>左边是分类任务的微调方式，输入将会同时送入Encoder和Decoder，最终使用最后一个输出为文本表示。</p></li>
<li><p>右边是翻译任务的微调方式，由于翻译任务的词表可能和模型词表不同，所以这里使用一个新的小型Encoder替换BART中的Embedding。</p></li>
</ul>
<figure>
<img src="BART微调.png" alt="BART微调">
<figcaption aria-hidden="true">BART微调</figcaption>
</figure>
<p>T5将分类任务和生成任务都视为生成式任务：</p>
<figure>
<img src="BART微调.png" alt="BART微调">
<figcaption aria-hidden="true">BART微调</figcaption>
</figure>
<h1 id="效果比较">效果比较</h1>
<p>对于理解任务，将两篇论文中实验结果整理为下表：</p>
<figure>
<img src="NLU效果比对.png" alt="NLU效果比对">
<figcaption aria-hidden="true">NLU效果比对</figcaption>
</figure>
<p>对于生成任务，两个模型都在CNN/DailyMail上进行了实验</p>
<figure>
<img src="NLG效果比对.png" alt="NLG效果比对">
<figcaption aria-hidden="true">NLG效果比对</figcaption>
</figure>
<p>综合比较来看，BART稍微好一些，尤其是在理解任务上。不过由于T5发布的模型比较大，参数量最多达到了11B，所以在GLUE和SuperGLUE上长期霸榜。</p>
<h1 id="其他细节">其他细节</h1>
<h2 id="位置编码">位置编码</h2>
<p>Transformers使用Position Encoding，使用sinusoidal函数</p>
<p>BERT和BART都换成了可学习的绝对位置嵌入</p>
<p>T5改成了相对位置嵌入(relative position embeddings)</p>
<h2 id="激活函数">激活函数</h2>
<p>Transformer最开始使用ReLU，BERT和GPT都使用GELU，BART也同样采用GELU，不过T5还是使用了最初的ReLU。</p>
<h2 id="模型大小">模型大小</h2>
<p>BART-large：12encoder, 12decoder, 1024hidden</p>
<p>T5-base：12encoder, 12decoder, 768 hidden, 220M parameters（2x
bert-base）</p>
<p>T5-large: 24encoder, 24decoder, 1024hidden, 770M parameters</p>
<p>T5-large的模型大小是BART-large的两倍。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">DB</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
